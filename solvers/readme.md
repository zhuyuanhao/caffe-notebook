* 对于稀疏数据，尽量使用学习率可自适应的优化方法，不用手动调节，而且最好采用默认值
* SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠
* 如果在意更快的收敛，并且需要训练较深较复杂的网络时，推荐使用学习率自适应的优化方法。
* Adadelta，RMSprop，Adam是比较相近的算法，在相似的情况下表现差不多。
* 在想使用带动量的RMSprop，或者Adam的地方，大多可以使用Nadam取得更好的效果